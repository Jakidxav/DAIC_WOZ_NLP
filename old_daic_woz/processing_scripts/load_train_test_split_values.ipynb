{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Jakidxav </br></br>\n",
    "\n",
    "Date: 04.24.2019 </br></br>\n",
    "\n",
    "We have been given 3 distinct subsets of the DAIC_WOZ data: the training, development, and testing sets. It has previously been split for other analyses, but we want to reconstruct the full dataset from these individual subsets. We want to do this so that we can do our own splitting of the data, especially in evaluating a given model through k-fold cross-validation.</br></br>\n",
    "\n",
    "We want to keep at least the following columns:\n",
    "- `Participant_ID`, \n",
    "- `PHQ8_Binary` scores for classification problems, and\n",
    "- the raw `PHQ8_Score` column for a regression analysis. \n",
    "\n",
    "We can also keep the `Gender` column for a different type of classification problem later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files on disk\n",
    "train_file = 'train_split_Depression_AVEC2017.csv'\n",
    "dev_file = 'dev_split_Depression_AVEC2017.csv'\n",
    "test_file = 'full_test_split.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in dataframes\n",
    "train = pd.read_csv(train_file)\n",
    "dev = pd.read_csv(dev_file)\n",
    "test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping gender column, might be interesting to examine later\n",
    "to_drop = ['PHQ8_NoInterest', 'PHQ8_Depressed', 'PHQ8_Sleep', 'PHQ8_Tired',\n",
    "       'PHQ8_Appetite', 'PHQ8_Failure', 'PHQ8_Concentrating', 'PHQ8_Moving']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unwanted columns\n",
    "#test set does not have these unwanted columns\n",
    "train = train.drop(to_drop, axis=1)\n",
    "dev = dev.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "#should have a total of 189 participants\n",
    "print(len(train)+len(dev)+len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Participant_ID', 'PHQ8_Binary', 'PHQ8_Score', 'Gender'], dtype='object')\n",
      "Index(['Participant_ID', 'PHQ8_Binary', 'PHQ8_Score', 'Gender'], dtype='object')\n",
      "Index(['Participant_ID', 'PHQ_Binary', 'PHQ_Score', 'Gender'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#notice that the columns are mislabeled between the train/dev and test sets\n",
    "print(train.columns)\n",
    "print(dev.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set testing set columns to match\n",
    "test.columns = train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataset containing all of the data\n",
    "#concatenate along rows\n",
    "full_dataset = pd.concat([train, dev, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "#check to see that all data is accounted for\n",
    "print(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dataframe by articipant id number\n",
    "full_dataset = full_dataset.sort_values(['Participant_ID'], ascending=True)\n",
    "\n",
    "#reset index\n",
    "full_dataset = full_dataset.reset_index(drop=True)\n",
    "\n",
    "#save to a CSV file\n",
    "full_dataset.to_csv('full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
