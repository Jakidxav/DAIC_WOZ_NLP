{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuther: Jakidxav\\nDate: 04.18.2019\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Auther: Jakidxav\n",
    "Date: 04.18.2019\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to test and demonstrate the helper methods I wrote as NLTK wrappers for the DAIC_WOZ NLP project. The goal of these functions is to clarify processing workflow by decluttering the workspace. All of the functions written in `clean_text.py` have the form: `output = function(input)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "from clean_text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example string\n",
    "example = \"<semantics> This mmm los angeles  &is [an] ExAmPlE? {of} 45 Jennifer string Americans. with.? <sigh> xxx [] punctuation michelle, [laugh] by stopwordS wouldn't I'm and whitespace.    Can't wait!!!!\"\n",
    "\n",
    "#create stop words set\n",
    "stops = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example processing flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whitespace tokenize\n",
    "tokens = tokenize(example)\n",
    "\n",
    "#take out semantic information\n",
    "no_semantics = remove_semantics(tokens)\n",
    "\n",
    "#remove proper nouns: NNP and NNPS are the POS tag for proper nouns\n",
    "no_nnp = remove_proper_nouns(no_semantics)\n",
    "\n",
    "#convert back to string to split apart contractions\n",
    "l2s = list_to_string(no_nnp)\n",
    "\n",
    "#split contractions AFTER removing semantic information\n",
    "split_contractions = split_contractions(l2s)\n",
    "\n",
    "#strip punctuation\n",
    "no_punct = strip_punctuation(split_contractions)\n",
    "\n",
    "#convert to lowercase\n",
    "lower = lower_case(no_punct)\n",
    "\n",
    "#remove stopwords\n",
    "no_stops = remove_stopwords(lower)\n",
    "\n",
    "#remove numbers from tokens\n",
    "words = remove_numbers(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Input:\n",
      " <semantics> This mmm los angeles  &is [an] ExAmPlE? {of} 45 Jennifer string Americans. with.? <sigh> xxx [] punctuation michelle, [laugh] by stopwordS wouldn't I'm and whitespace.    Can't wait!!!! \n",
      "\n",
      "Tokenize:\n",
      " ['<semantics>', 'This', 'mmm', 'los', 'angeles', '&is', '[an]', 'ExAmPlE?', '{of}', '45', 'Jennifer', 'string', 'Americans.', 'with.?', '<sigh>', 'xxx', '[]', 'punctuation', 'michelle,', '[laugh]', 'by', 'stopwordS', \"wouldn't\", \"I'm\", 'and', 'whitespace.', \"Can't\", 'wait!!!!'] \n",
      "\n",
      "Remove Semantic Info:\n",
      " ['This', 'mmm', 'los', 'angeles', '&is', 'ExAmPlE?', '{of}', '45', 'Jennifer', 'string', 'Americans.', 'with.?', 'xxx', 'punctuation', 'michelle,', 'by', 'stopwordS', \"wouldn't\", \"I'm\", 'and', 'whitespace.', \"Can't\", 'wait!!!!'] \n",
      "\n",
      "Remove Proper Nouns:\n",
      " ['This', 'mmm', 'los', 'angeles', '&is', '45', 'string', 'with.?', 'punctuation', 'michelle,', 'by', 'stopwordS', \"wouldn't\", 'and', 'whitespace.', 'wait!!!!'] \n",
      "\n",
      "Split Contractions:\n",
      " ['This', 'mmm', 'los', 'angeles', '&', 'is', '45', 'string', 'with', '.?', 'punctuation', 'michelle', ',', 'by', 'stopwordS', 'wouldn', \"'\", 't', 'and', 'whitespace', '.', 'wait', '!!!!'] \n",
      "\n",
      "Remove Punctuation:\n",
      " ['This', 'mmm', 'los', 'angeles', '', 'is', '45', 'string', 'with', '', 'punctuation', 'michelle', '', 'by', 'stopwordS', 'wouldn', '', 't', 'and', 'whitespace', '', 'wait', ''] \n",
      "\n",
      "Lowercase:\n",
      " ['this', 'mmm', 'los', 'angeles', '', 'is', '45', 'string', 'with', '', 'punctuation', 'michelle', '', 'by', 'stopwords', 'wouldn', '', 't', 'and', 'whitespace', '', 'wait', ''] \n",
      "\n",
      "Remove Stopwords:\n",
      " ['los', 'angeles', '', '45', 'string', '', 'punctuation', 'michelle', '', 'stopwords', '', 'whitespace', '', 'wait', ''] \n",
      "\n",
      "Final:\n",
      " ['los', 'angeles', 'string', 'punctuation', 'michelle', 'stopwords', 'whitespace', 'wait'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Raw Input:\\n', example, '\\n')\n",
    "print('Tokenize:\\n', tokens, '\\n')\n",
    "print('Remove Semantic Info:\\n', no_semantics, '\\n')\n",
    "print('Remove Proper Nouns:\\n', no_nnp, '\\n')\n",
    "print('Split Contractions:\\n', split_contractions, '\\n')\n",
    "print('Remove Punctuation:\\n', no_punct, '\\n')\n",
    "print('Lowercase:\\n', lower, '\\n')\n",
    "print('Remove Stopwords:\\n', no_stops, '\\n')\n",
    "print('Final:\\n', words, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
